{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d95913a-67ee-49a2-9e19-92a87804752b",
   "metadata": {},
   "source": [
    "# DO NOT RUN THIS COPY; an exact version is running locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52921339-2c14-4c2c-b521-86916bedf060",
   "metadata": {},
   "source": [
    "# We first import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8df777f-cd57-4aed-8539-16b9cc2ee080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "from gemma_instruct import llm as gemma    # This is a new model by Google, I am running this locally\n",
    "from openai_chatgpt import chatgpt         # This is a chatGPT-3.5 by OpenAI, I am running this using API calls\n",
    "import os\n",
    "from generateVectorDB import get_collection, process_pdf  # These functions help me store pdf texts as vectors\n",
    "from retriever import get_documents_by_query              # This function helps me retrieve the information from the database\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca084e3-1d8c-42f0-b525-5d279bd8c017",
   "metadata": {},
   "source": [
    "# Database path setup and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad0cd5-36e0-40bf-9559-968dc30aca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dir = \"./DB/\"\n",
    "\n",
    "collection = get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59fd4a-c9e3-47f6-a598-c4593ee5b853",
   "metadata": {},
   "source": [
    "# We will create a sidebar for file upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b979c6b-828d-46c6-b88a-5a1067fa05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sidebar with streamlit\n",
    "sidebar = st.sidebar\n",
    "with sidebar.form(key='file-handler', clear_on_submit=True):\n",
    "    # Create a file uploading module\n",
    "    uploaded_file = st.file_uploader(label=\"Upload your doc\", type=\"pdf\",\n",
    "                                     accept_multiple_files=False, key=\"pdf_upload\")\n",
    "    submitted = st.form_submit_button(\"Upload\")\n",
    "    if uploaded_file and submitted:\n",
    "        file_name = os.sep.join([upload_dir, uploaded_file.name])\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(uploaded_file.getbuffer())\n",
    "        # once file is uploaded, process the pdf using internal functions and logic\n",
    "        # In our case, we use tessaract to run OCR on each pdf pages.\n",
    "        # We read the pdf pages as images\n",
    "        process_pdf(file_name, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43758275-4802-49f8-8585-237cc4349093",
   "metadata": {},
   "source": [
    "# Some setup to display the messages properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f5c81-80da-4984-8359-b293ee46c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a621a-93da-4ca0-82b2-046a6e310846",
   "metadata": {},
   "source": [
    "# Create a choice to use local model or send data to OpenAI servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bb5f6-1ae6-4b7d-aee9-9d0b9af67697",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_choice = st.radio(\n",
    "    \"Which bot would you like to use?\",\n",
    "    [\":zany_face: LocalGPT\", \":sunglasses: ChatGPT\"],\n",
    "    captions=[\"gemma\", \"chatGPT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cf92d-9390-4c31-a09e-c9feb2de71eb",
   "metadata": {},
   "source": [
    "# Take input and process it according to the choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed57ff-f030-4f69-ae64-ac94b7467e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input from the user comes from the chat input function\n",
    "if query := st.chat_input(\"Ask a question!\"):\n",
    "    # Add it to the list of messages\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(query)\n",
    "        st.session_state.messages.append({\"role\": \"user\",\n",
    "                                          \"message\": query})\n",
    "\n",
    "    # Process the query by sending it to the selected bot\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Bot thinking ...\"):\n",
    "            if bot_choice == \":zany_face: LocalGPT\":\n",
    "                # For local bot, process everything using the local gemma bot\n",
    "\n",
    "                # First step is to make the user query more relevant such that we can retrieve good quality documents\n",
    "                alternate_search_query = gemma(f\"Rewrite the following question encase in backticks, \"\n",
    "                                               f\"adding more context, for retrieving information.\\n`{query}`\")\n",
    "\n",
    "                # Once we have the documents, combine them together\n",
    "                context_docs = '===========\\n'.join(get_documents_by_query(alternate_search_query, collection))\n",
    "                print(context_docs)\n",
    "\n",
    "                # Send the documents and the question to the bot again, to generate an answer\n",
    "                bot_response = gemma(f\"Given the following pages, answer the question to the best of your \"\n",
    "                                     f\"ability.\\n\\nPages:\\n{context_docs}\\n\\nQuestion: {query}\")\n",
    "\n",
    "            else:\n",
    "                # pretty much the same steps but with ChatGPT\n",
    "                alternate_search_query = chatgpt(f\"Rewrite the following question encase in backticks, \"\n",
    "                                                 f\"adding more context, for retrieving information.\\n`{query}`\")\n",
    "                context_docs = '===========\\n'.join(get_documents_by_query(alternate_search_query, collection))\n",
    "                print(context_docs)\n",
    "                bot_response = chatgpt(f\"Given the following pages, answer the question to the best of your \"\n",
    "                                       f\"ability.\\n\\nPages:\\n{context_docs}\\n\\nQuestion: {query}\")\n",
    "\n",
    "        st.markdown(bot_response)\n",
    "        st.session_state.messages.append({\"role\": \"assistant\",\n",
    "                                          \"message\": bot_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d33953-11cd-4ba3-8af8-e274e4914e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
